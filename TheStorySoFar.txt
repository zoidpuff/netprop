

A brief overview of the things that I have done so far in the project

First thing I did was to test different normalization stratgies for netpropgation vectors.
This included test the recovery AUC for four normalization approaches.
    - Eigenvector normalization with/without log ratios
        The log ratio approach seemed to be necessity as the probilities are very low/close to zero and
        and as such to not lend well to being divided by. This is also kinda a nice approach because 
        it tranform the probilities into log odds ratios, which are common.
    - Naive Permutation testing
        To achive this, I had to change the method used for page rank from prpack to arpack, as prpack
        doesnt handle a damping factor of 1. I also had to make an effort to ensure that the solutions
        given by arpack are good, as they seem to not be deterministic. To this end I am sure that 
        changing the seed vector makes no differece for the damping = 1 case, so in theory I could just compute
        the eigen centraility vector once, since that graph is always the same.
        The "correct implmentation" I wrote to do this very slow and didnt paralllize well.
        I decided to do a less precise appraoach were I dont keep track of selected genes.
        In theory, it should make no difference for large sampling size but alas. 
    - Degree informed Permutation testing
        This was bit of a mess. I first just did the method that was described in a paper which consisited 
        of sorting the nodes in buckets based on node degree and then sampling seed genes from the buckets.
        I found this to be a problem for very high degree nodes, as they are very few, and during smapling, they
        would pretty much always be picked, leading to problems when computing the ratios. To combat this, 
        I devised a method that probiliscally sorted the nodes into buckets. This was fairly convoluted and
        and theyre were so may edge cases. I think If I want to revisit this I should rewrite the whole method.
    - Z- scoring
        I didnt really test this but Im pretty sure this makes no sense.

To test these normalization methods, I computed the network propgation for traits, randomly leaving
out some seed genes and seeing if they were ranked high in the netpropgation vecotors.
The results of this were kinda mixed and seem to be dependant on the scores used.
The Eigenvector normalization approach seemed to perform on par with the Permutation testing.
The degree informed permutation testing method I wrote was overly convoluted and I dont really trust implmentation
but it did perfom worese then just normal permutation testing. The oddest result was that when I used the 
overall scores from opentargets, the unnormalized method performed, by far the best. 
We theorized that this was likely due to the fact that there is a higher correlation with node degree and 
overall score, and as such, as high degree nodes are punished with the normalization approaches, they would 
be downranked. This highlightes a potential problem with these approaches which is over correction.
One avenue to look at is might be to change the eignevector centraility appraoch so that it considers seed locality
better (i.e. changing the damping factor to 0.99 instead of 1). 

The next avenue I went down was to test different distance funtions and embedding methods for the netprop vectors
The first part of that entailed building a list of "related traits" that could be used to evaluate different metrics
To do this, I imported the disease ontology graph from the diseases file from opentargets. 
My definition of related, was the following, If leaf node, diseases are related to their parents and sibilings.
If they are internal nodes, They are related to all descendants. This approach has its problems, espcially when the 
the diseases are very high level and have many descendant (e.g. genetic disease).
To combat this, I prefilter out nodes that are overly broad, by removing nodes that have more then 30 edges
from the graph. This is not ideal, and this did end up removing some nodes that might be valuable, but on the whole
I think this strategy is good enough.

A brief avenue I went down related to this was to trying to compare the sementic similarity if diseasesb by using 
a sentance tranformer trained on pubmed articles. To do this I used a colab notebook and and the sentance
transformer library. I created strings which consisted of the name and the discription of the diseases.
I then compared these embedding to the netpropagtion embeddings by looking at cosimilarity.
Perhaps I should just use correlation. This yielded a corrlaiton of around 0.4 but I still need to do this more rigoursly

Using pairs of related traits I divised a metric to compare the qualities of the different distance functions.
This metric is simply the AUROC of the distance with the true and false classes being related and not related 
and the prediction metric being the distances. I then did a multi facteted experinment where I compared the distance 
metrics. I also looked at the impact of removing low variance genes from the input. There are some nuances to this that I will just list off with out going into detail:
    - Similiary metric must be converted into distances, not straighforward
    - Kendall distance is very slow to compute
    - Jensen shannon divergence has problems with zero values
    - 

